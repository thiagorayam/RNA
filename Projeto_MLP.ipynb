{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Projeto - MLP",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thiagorayam/RNA/blob/main/Projeto_MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rF4ib4a7QdM8"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "#MLP\n",
        "\n",
        "class MLP(object):\n",
        "\n",
        "#Parâmetros da rede \n",
        "    def __init__(self, eta = 0.1, epoch =10000, epsilon=0.01, alfa=0, i=13, j=3, k=3, l=0, nlayer=1):\n",
        "        self.eta = eta # taxa de aprendizagem\n",
        "        self.epoch = epoch # número máximo de épocas\n",
        "        self.epsilon = epsilon # erro médio admissível\n",
        "        self.alfa=alfa # fator do termo momentum\n",
        "        self.i=i # número de neurônios na camada de entrada\n",
        "        self.j=j # número de neurônios na camada j\n",
        "        self.k=k # número de neurônios na camada k\n",
        "        self.l=l # número de neurônios na camada l\n",
        "        self.nlayer= nlayer #número de camadas intermediárias\n",
        "\n",
        "#Inicializa os pesos (matriz Wji) e bias (vetor teta_j) da rede. j,i representam os números de neurônios da camada de saída e entrada    \n",
        "    def inicialize(self): \n",
        "\n",
        "        self.w_ji=np.random.uniform(-1,1, [self.j,self.i+1])\n",
        "        self.w_kj=np.random.uniform(-1,1, [self.k,self.j+1])\n",
        "\n",
        "        if (self.nlayer>1):\n",
        "          self.w_lk=np.random.uniform(-1,1, [self.l,self.k+1])\n",
        "\n",
        "        return self\n",
        "\n",
        "#Função de ativação utilizada - sigmoide\n",
        "    def sigm(self, x):\n",
        "        return 1/(1 + np.exp(-x))\n",
        "\n",
        "#Derivada da função sigmoide\n",
        "    def d_sigm(self, x):\n",
        "        return x * (1.0 - x)\n",
        "\n",
        "#Cálculo da saída da camada escondida e da saída      \n",
        "    def feed_net(self, y_i):\n",
        "        self.y_i=y_i #entrada da camada i\n",
        "        self.y_j=self.sigm(np.dot(self.w_ji,np.concatenate((y_i,[1])))) #entrada da camada j, termo adicionado devido ao bias\n",
        "        self.y_k=self.sigm(np.dot(self.w_kj,np.concatenate((self.y_j,[1])))) #entrada da camada K, termo adicionado devido ao bias\n",
        "\n",
        "        if (self.nlayer>1):\n",
        "          self.y_l=self.sigm(np.dot(self.w_lk,np.concatenate((self.y_k,[1])))) #entrada da camada L, termo adicionado devido ao bias\n",
        "        \n",
        "        return self\n",
        "\n",
        "#Cálculo da saída da camada escondida e da saída \n",
        "    def back_propagation(self, error):\n",
        "        \n",
        "        if (self.nlayer>1):\n",
        "          self.delta_l=error*self.d_sigm(self.y_l) #Gradiente local do erro na camada de saída\n",
        "          self.delta_k=np.dot(self.w_lk.T,self.delta_l)*self.d_sigm(np.concatenate((self.y_k,[1])))#Gradiente local do erro na camada escondida\n",
        "          self.delta_k=self.delta_k[0:self.k] # O último elemento representava apenas a relação do erro do bias, mas o bias não se interliga com a camada anterior.\n",
        "          \n",
        "        else:\n",
        "          self.delta_k=error*self.d_sigm(self.y_k) #Gradiente local do erro na camada de saída\n",
        "        \n",
        "        self.delta_j=np.dot(self.w_kj.T,self.delta_k)*self.d_sigm(np.concatenate((self.y_j,[1]))) #Gradiente local do erro na camada escondida\n",
        "        self.delta_j=self.delta_j[0:self.j] # O último elemento representava apenas a relação do erro do bias, mas o bias não se interliga com a camada anterior.  \n",
        "\n",
        "        return self\n",
        "\n",
        "#Atualização dos pesos.\n",
        "    def fit(self, X, y):\n",
        "        \n",
        "        emed=1.01\n",
        "        count=0 \n",
        "        self.inicialize()\n",
        "        deltaw_lk=0\n",
        "        deltaw_kj=0\n",
        "        deltaw_ji=0\n",
        "\n",
        "        plt.axis([0, self.epoch, 0, 0.2])\n",
        "        \n",
        "        while (emed > self.epsilon and count < self.epoch):\n",
        "         \n",
        "         index = np.arange(X.shape[0])\n",
        "         np.random.shuffle(index)\n",
        "         X = X[index]\n",
        "         y = y[index]\n",
        "\n",
        "         emed=0\n",
        "         \n",
        "         for n in range (np.size(X, 0)):\n",
        "\n",
        "           y_i=X[n,:]\n",
        "           self.feed_net(y_i)      \n",
        "           \n",
        "           if (self.nlayer>1):\n",
        "             error=y[n,:]-self.y_l\n",
        "           else:\n",
        "             error=y[n,:]-self.y_k\n",
        "                       \n",
        "           emed += np.sum(np.square(error))\n",
        "           self.back_propagation(error) \n",
        "\n",
        "           if (self.nlayer>1):\n",
        "             self.w_lk=self.w_lk+self.eta*self.delta_l[:, np.newaxis]*np.concatenate((self.y_k,[1])) + self.alfa*deltaw_lk\n",
        "          \n",
        "           self.w_kj=self.w_kj+self.eta*self.delta_k[:, np.newaxis]*np.concatenate((self.y_j,[1])) + self.alfa*deltaw_kj\n",
        "           self.w_ji=self.w_ji+self.eta*self.delta_j[:, np.newaxis]*np.concatenate((self.y_i,[1]))+ self.alfa*deltaw_ji\n",
        "\n",
        "           if (self.nlayer>1):\n",
        "             deltaw_lk=self.eta*self.delta_l[:, np.newaxis]*np.concatenate((self.y_k,[1])) + self.alfa*deltaw_lk\n",
        "           \n",
        "           deltaw_kj=self.eta*self.delta_k[:, np.newaxis]*np.concatenate((self.y_j,[1])) + self.alfa*deltaw_kj\n",
        "           deltaw_ji=-self.eta*self.delta_j[:, np.newaxis]*np.concatenate((self.y_i,[1])) + self.alfa*deltaw_ji\n",
        "           \n",
        "           self.feed_net(y_i)\n",
        "          \n",
        "         emed /= np.size(X, 0)\n",
        "         self.emed=emed\n",
        "         plt.scatter(count,emed)\n",
        "         count += 1\n",
        "\n",
        "        print(count)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        ypred=[]\n",
        "        for n in range (np.size(X, 0)):\n",
        "          y_i=X[n,:]\n",
        "          self.feed_net(y_i)\n",
        "\n",
        "          if (self.nlayer>1):\n",
        "            ypred.append(self.y_l)\n",
        "          else: \n",
        "            ypred.append(self.y_k)\n",
        "\n",
        "        return np.array(ypred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ih-I73ImQ1wP"
      },
      "source": [
        "def compute_accuracy(a, b): #a = pred , b = y\n",
        "  hits = 0\n",
        "  for i in range(a.shape[0]):\n",
        "    linha_a = a[i]\n",
        "    linha_b = b[i]\n",
        "    c = np.sum(linha_a[:min(len(linha_a), len(linha_b))] == linha_b[:min(len(linha_a), len(linha_b))])\n",
        "    if c == len(linha_a):\n",
        "      hits = hits + 1\n",
        "  acc = hits / a.shape[0]\n",
        "  return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92o_otzFRSX2"
      },
      "source": [
        "# Classificação Wine Data\n",
        "\n",
        "N=178\n",
        "dim=13\n",
        "\n",
        "X=np.zeros((N, dim))\n",
        "y=np.zeros(N)\n",
        "ref_arquivo = open(\"/content/sample_data/wine.data\",\"r\")\n",
        "for j in range (N):\n",
        "  linha = ref_arquivo.readline()\n",
        "  valores=linha.split(',')\n",
        "  y[j]=valores[0]\n",
        "  for i in range(dim):\n",
        "    X[j,i]=valores[i+1]\n",
        "\n",
        "X=X/X.max(axis=0)\n",
        "\n",
        "arr = np.zeros((len(y), 3))\n",
        "for i in range(len(y)):\n",
        "  c = int(y[i])-1\n",
        "  arr[i][c] = 1\n",
        "y=arr\n",
        "\n",
        "train_rate=0.75\n",
        "\n",
        "index = np.arange(X.shape[0])\n",
        "np.random.shuffle(index)\n",
        "X = X[index]\n",
        "y = y[index]\n",
        "X_train=X[0:int(N*train_rate)]\n",
        "X_test=X[0:int(N*(1-train_rate))]\n",
        "y_train=y[0:int(N*train_rate)]\n",
        "y_test=y[0:int(N*(1-train_rate))]\n",
        "\n",
        "mlp = MLP(eta=0.2, epoch=500, epsilon=0.005, alfa=0.2, i=dim, j=10, k=3, l=2, nlayer=1)\n",
        "mlp.fit(X_train,y_train)\n",
        "ypred=np.around(mlp.predict(X_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6usrnkapRwWF"
      },
      "source": [
        "# Regressão - Geographical Original of Music Data Set\n",
        "\n",
        "N=1059\n",
        "dim=68\n",
        "\n",
        "X=np.zeros((N, dim))\n",
        "y=np.zeros((N,2))\n",
        "ref_arquivo = open(\"/content/sample_data/default_features_1059_tracks.txt\",\"r\")\n",
        "for j in range (N):\n",
        "  linha = ref_arquivo.readline()\n",
        "  valores=linha.split(',')\n",
        "  y[j,0]=valores[68]\n",
        "  y[j,1]=valores[69]\n",
        "  for i in range(dim):\n",
        "    X[j,i]=valores[i+1]\n",
        "\n",
        "Xmax=X.max(axis=0)\n",
        "X=X/Xmax\n",
        "ymax=y.max(axis=0)\n",
        "y=y/ymax\n",
        "\n",
        "train_rate=0.75\n",
        "\n",
        "index = np.arange(X.shape[0])\n",
        "np.random.shuffle(index)\n",
        "X = X[index]\n",
        "y = y[index]\n",
        "X_train=X[0:int(N*train_rate)]\n",
        "X_test=X[0:int(N*(1-train_rate))]\n",
        "y_train=y[0:int(N*train_rate)]\n",
        "y_test=y[0:int(N*(1-train_rate))]\n",
        "\n",
        "mlp = MLP(eta=1, epoch=500, epsilon=0.005, alfa=0, i=68, j=34, k=2, l=2, nlayer=1)\n",
        "mlp.fit(X_train,y_train)\n",
        "ypred=mlp.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}